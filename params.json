{"name":"Visco","tagline":"Hadoop with a modified framework to allow for concurrent execution of the Shuffle and Reduce phases.","body":"## Background\r\n\r\nHadoop is one of the most high-profile and well known open-source projects for distributed systems, being used on a daily basis by some of the largest technology and Internet companies such as Google and Microsoft. Yahoo! successfully used it rebuild their search indexes and in 2008, Hadoop broke the world record to become the fastest system to sort a terabyte of data, doing so in just under 3.5 minutes.\r\n\r\nA major design decision in the MapReduce philosophy was to be able to best utilise commodity hardware and not need high performance computers. At over 178,000 lines of code and over 5,000 Java classes, the complexity and vastness of Hadoop has led it to be described as an \"alien language\".\r\n\r\nA MapReduce job consists of three stages - Map, Shuffle and Reduce. A canonical example of using this paradigm is to count the repetition of words in a document. The stages perform the following operations:\r\n\r\n`Map` - servers read the input data provided and output a set of (key, value) pairs.\r\n\r\n`Shuffle` - intermediate results are distributed across all other servers, such that all keys falling in a given range end up on the same server.\r\n\r\n`Reduce` - all values for a given key are aggregated and a final result is generated.\r\n\r\n## History\r\n\r\nVisco spawned from an MSc Computing Science group project at [Imperial College London](http://www3.imperial.ac.uk/) put forward by [Dr. Paolo Costa](http://research.microsoft.com/en-us/um/people/pcosta/). \r\n\r\n## Collaborators\r\n\r\nAkram Hussein (@akramhussein), Alexandros Milaios (@am6010), Andreas Williams (@agouil), Emmanouil Matsis (@emmanouilmatsis), Jason Fong (@jonojace), Pavlos Mitsoulis-Ntompos (@pm3310) and Konstantinos Kloudas (@kl0u) (external).\r\n\r\n## Objective\r\n\r\nTo introduce new functionality to the Hadoop codebase (version 1.0.1) that would enable the Map tasks that are completed to be immediately passed to the Reduce stage as opposed to the current method which can only pass 5 tasks at a time. This causes much of the data to be spilled on to disk if the memory is full, a costly performance drawback. Effectively, an overlap of the Shuffle and Reduce stages will occur, therefore delivering a faster runtime and improved network load balancing from the reduced transfer of data across the network.\r\n\r\n## Design\r\n\r\nIn order to improve the overall performance of the framework it was necessary to implement a data structure that could aggregate results in a parallel fashion. It was decided that a merging binary tree data structure would be used and integrated in to the Map and Reduce stages adding a new phase in both of them. \r\n\r\n> Visco Merging Tree\r\n\r\n![Visco Merging Tree](https://raw.github.com/akramhussein/visco/gh-pages/images/merging-tree.png)\r\n\r\nThe first part of this new phase takes part after the completion of the map task; merging the various map outputs on the same key and storing these partial results on disk. The reduce tasks, then pull the partial outputs from the different map tasks, which are then merged again on the same key, and are processed by the reduce function as long as they are merged. \r\n\r\nThus, the Reduce and the Merging phase can run concurrently and more efficiently as there is no need to wait for the traditional Shuffle-Sort phase to complete and then begin the reduce computation. With this addition, we also achieve more efficient computation times, because as long as the Map phase is completed the intermediate data can be almost directly fed in the reduce function without waiting for the Shuffle- Sort phase to finish, which in some cases adds significant delay to the execution time, as argued above.\r\n\r\n> Visco Pipeline\r\n\r\n![Visco Pipeline](https://raw.github.com/akramhussein/visco/gh-pages/images/nodes.png)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}